{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from chromadb) (2.10.6)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.21.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from chromadb) (0.21.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from chromadb) (0.15.2)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.16-cp310-cp310-win_amd64.whl.metadata (42 kB)\n",
      "Collecting httpx>=0.27.0 (from chromadb)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb)\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting anyio (from httpx>=0.27.0->chromadb)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: sympy in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro>=1.5.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.29.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.4-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.2.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.27.0->chromadb)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "   ---------------------------------------- 0.0/611.1 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 262.1/611.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 611.1/611.1 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl (150 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading grpcio-1.71.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.5/4.3 MB 2.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.8/4.3 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.3/4.3 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.8/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.6/4.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.8/2.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading mmh3-5.1.0-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Downloading onnxruntime-1.21.0-cp310-cp310-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.8 MB 4.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/11.8 MB 5.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/11.8 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/11.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/11.8 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.9/11.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.2/11.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 5.4 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.31.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl (183 kB)\n",
      "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\n",
      "Downloading orjson-3.10.16-cp310-cp310-win_amd64.whl (133 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Downloading watchfiles-1.0.4-cp310-cp310-win_amd64.whl (284 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-win_amd64.whl (176 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53914 sha256=d91838aff7251153e2f342afac1e45c30b816f4afc2298072226a12241a163fd\n",
      "  Stored in directory: c:\\users\\darsh\\appdata\\local\\pip\\cache\\wheels\\e1\\26\\51\\d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, websockets, websocket-client, tomli, tenacity, sniffio, python-dotenv, pyreadline3, pyproject_hooks, pyasn1, protobuf, overrides, orjson, opentelemetry-util-http, oauthlib, mmh3, importlib-resources, httptools, h11, grpcio, distro, deprecated, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, uvicorn, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-proto, opentelemetry-api, humanfriendly, httpcore, googleapis-common-protos, build, anyio, watchfiles, starlette, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, httpx, google-auth, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Successfully installed anyio-4.9.0 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 deprecated-1.2.18 distro-1.9.0 durationpy-0.9 fastapi-0.115.12 flatbuffers-25.2.10 google-auth-2.38.0 googleapis-common-protos-1.69.2 grpcio-1.71.0 h11-0.14.0 httpcore-1.0.7 httptools-0.6.4 httpx-0.28.1 humanfriendly-10.0 importlib-resources-6.5.2 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.21.0 opentelemetry-api-1.31.1 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-sdk-1.31.1 opentelemetry-semantic-conventions-0.52b1 opentelemetry-util-http-0.52b1 orjson-3.10.16 overrides-7.7.0 posthog-3.23.0 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 python-dotenv-1.1.0 requests-oauthlib-2.0.0 rsa-4.9 sniffio-1.3.1 starlette-0.46.1 tenacity-9.0.0 tomli-2.2.1 uvicorn-0.34.0 watchfiles-1.0.4 websocket-client-1.8.0 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.3\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "print(chromadb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# Load InLegalBERT Model\n",
    "model_name = \"law-ai/InLegalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB (persistent mode)\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create collection\n",
    "collection = chroma_client.get_or_create_collection(\"legal_docs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Encodes the input text using InLegalBERT and returns a fixed-size embedding.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Extract CLS token\n",
    "    return embedding.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Found 12 batch files!\n",
      "\n",
      "🚀 Processing processed_batch_0.parquet (1/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 1: 100%|██████████| 1000/1000 [01:19<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_0.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_1.parquet (2/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 2: 100%|██████████| 1000/1000 [01:27<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_1.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_10.parquet (3/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 3: 100%|██████████| 1000/1000 [01:33<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_10.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_11.parquet (4/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 4: 100%|██████████| 970/970 [01:14<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_11.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_2.parquet (5/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 5: 100%|██████████| 1000/1000 [01:29<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_2.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_3.parquet (6/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 6: 100%|██████████| 1000/1000 [01:22<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_3.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_4.parquet (7/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 7: 100%|██████████| 1000/1000 [01:35<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_4.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_5.parquet (8/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 8: 100%|██████████| 1000/1000 [01:35<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_5.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_6.parquet (9/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 9: 100%|██████████| 1000/1000 [01:23<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_6.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_7.parquet (10/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 10: 100%|██████████| 1000/1000 [01:59<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_7.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_8.parquet (11/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 11: 100%|██████████| 1000/1000 [01:19<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_8.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_9.parquet (12/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Batch 12: 100%|██████████| 1000/1000 [01:25<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ processed_batch_9.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🎉 All batches processed & stored successfully in ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ✅ Get all batch files (assuming they are named like processed_batch_0.parquet, processed_batch_1.parquet, etc.)\n",
    "batch_files = sorted(glob.glob(\"processed_batch_*.parquet\"))\n",
    "print(f\"📂 Found {len(batch_files)} batch files!\")\n",
    "\n",
    "# ✅ Process each batch file separately\n",
    "for batch_id, batch_file in enumerate(batch_files):\n",
    "    print(f\"\\n🚀 Processing {batch_file} ({batch_id + 1}/{len(batch_files)})...\")\n",
    "\n",
    "    # Load batch (only one at a time)\n",
    "    batch_df = pd.read_parquet(batch_file)\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_texts = []\n",
    "    all_ids = []\n",
    "\n",
    "    for i, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Embedding Batch {batch_id + 1}\"):\n",
    "        text = row[\"cleaned_text\"]\n",
    "        pos = row[\"pos_tags\"]\n",
    "        ner = row[\"named_entities\"]\n",
    "\n",
    "        # Combine text + POS + NER\n",
    "        combined_text = f\"{text} POS: {' '.join([t[1] for t in pos])} NER: {' '.join([t[1] for t in ner])}\"\n",
    "        \n",
    "        # Compute embedding\n",
    "        embedding = get_embedding(combined_text)\n",
    "        \n",
    "        # Store for ChromaDB\n",
    "        all_embeddings.append(embedding.tolist())\n",
    "        all_texts.append(text)\n",
    "        all_ids.append(f\"batch_{batch_id}_{i}\")\n",
    "\n",
    "    # Store batch in ChromaDB\n",
    "    collection.add(ids=all_ids, documents=all_texts, embeddings=all_embeddings)\n",
    "\n",
    "    print(f\"✅ {batch_file} stored in ChromaDB successfully!\")\n",
    "\n",
    "print(\"\\n🎉 All batches processed & stored successfully in ChromaDB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total documents in ChromaDB: 11970\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Total documents in ChromaDB: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 **Top Similar Cases:**\n",
      "1. JUDGMENT C.K. Mahajan, J. 1. This Civil Revision raises an interesting question of law with regard to landlord-tenant relationship in the post Rent Act era. 2. Premises No. L-3 Kanchanjunga, 18 Barakh...\n",
      "2. JUDGMENT C.K. Mahajan, J. 1. This Civil Revision raises an interesting question of law with regard to landlord-tenant relationship in the post Rent Act era. 2. Premises No. L-3 Kanchanjunga, 18 Barakh...\n",
      "3. PETITIONER: B. BANERJEE Vs. RESPONDENT: ANITA PAN DATE OF JUDGMENT20/11/1974 BENCH: KRISHNAIYER, V.R. BENCH: KRISHNAIYER, V.R. BEG, M. HAMEEDULLAH GOSWAMI, P.K. CITATION: 1975 AIR 1146 1975 SCR (2) 77...\n",
      "4. JUDGMENT M.M. Kumar, J. 1. This is tenant's petition filed under Section 15(6) of the Haryana Urban (Control of Rent and Eviction) Act, 1973, challenging order of reversal dated 11.3.1999, passed by t...\n",
      "5. IN THE COURT OF SHRI M. P. SINGH: SENIOR CIVIL JUDGE : RENT CONTROLLER: KARKARDOOMA COURTS (EAST), DELHI Suit No. 291/11 Unique Case ID No. 02402C288882011 Sh. Raj Bhushan Gupta, S/o Sh. Ram Kishore G...\n"
     ]
    }
   ],
   "source": [
    "def search_similar_cases(query_text, top_k=5):\n",
    "    \"\"\"Finds the most similar cases in ChromaDB given a query.\"\"\"\n",
    "    query_embedding = get_embedding(query_text).tolist()\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    print(\"\\n🔍 **Top Similar Cases:**\")\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        print(f\"{i+1}. {doc[:200]}...\")  # Show first 200 characters of case text\n",
    "\n",
    "# Example query\n",
    "search_similar_cases(\"tenant eviction under Section 39 of Delhi Rent Control Act\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darsh\\anaconda3\\envs\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Load InLegalBERT tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "\n",
    "# ✅ Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"legal_sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(tokenized_sentence):\n",
    "    \"\"\"Convert tokenized sentence back to readable text.\"\"\"\n",
    "    return tokenizer.decode(tokenized_sentence, skip_special_tokens=True)\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generate embeddings using InLegalBERT.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Extract CLS token embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Found 12 batch files!\n",
      "\n",
      "🚀 Processing processed_batch_0.parquet (1/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 1: 100%|██████████| 1000/1000 [08:52<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_0.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_1.parquet (2/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 2: 100%|██████████| 1000/1000 [05:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_1.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_10.parquet (3/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 3: 100%|██████████| 1000/1000 [05:42<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_10.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_11.parquet (4/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 4: 100%|██████████| 970/970 [12:17<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_11.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_2.parquet (5/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 5: 100%|██████████| 1000/1000 [08:06<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_2.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_3.parquet (6/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 6: 100%|██████████| 1000/1000 [06:13<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_3.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_4.parquet (7/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 7: 100%|██████████| 1000/1000 [06:20<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_4.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_5.parquet (8/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 8: 100%|██████████| 1000/1000 [07:18<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_5.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_6.parquet (9/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 9: 100%|██████████| 1000/1000 [08:58<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_6.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_7.parquet (10/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 10: 100%|██████████| 1000/1000 [17:19<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_7.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_8.parquet (11/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 11: 100%|██████████| 1000/1000 [1:17:23<00:00,  4.64s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_8.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🚀 Processing processed_batch_9.parquet (12/12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Sentences in Batch 12: 100%|██████████| 1000/1000 [06:57<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence embeddings from processed_batch_9.parquet stored in ChromaDB successfully!\n",
      "\n",
      "🎉 All sentence embeddings processed & stored successfully in ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Get all batch files (processed_batch_*.parquet)\n",
    "batch_files = sorted(glob.glob(\"processed_batch_*.parquet\"))\n",
    "print(f\"📂 Found {len(batch_files)} batch files!\")\n",
    "\n",
    "# ✅ Process each batch file separately\n",
    "for batch_id, batch_file in enumerate(batch_files):\n",
    "    print(f\"\\n🚀 Processing {batch_file} ({batch_id + 1}/{len(batch_files)})...\")\n",
    "\n",
    "    # ✅ Load batch\n",
    "    batch_df = pd.read_parquet(batch_file)\n",
    "\n",
    "    all_sentence_embeddings = []\n",
    "    all_sentence_texts = []\n",
    "    all_sentence_ids = []\n",
    "\n",
    "    # ✅ Process each case in the batch\n",
    "    for i, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Embedding Sentences in Batch {batch_id + 1}\"):\n",
    "        case_id = f\"batch_{batch_id}_{i}\"  # ✅ Unique identifier for case-level mapping\n",
    "        tokenized_sentences = row[\"tokenized_text\"]  # ✅ List of tokenized sentences\n",
    "\n",
    "        # ✅ Process each tokenized sentence\n",
    "        for idx, tokenized_sentence in enumerate(tokenized_sentences):\n",
    "            decoded_text = decode_tokens(tokenized_sentence)  # Convert back to text\n",
    "            embedding = get_embedding(decoded_text)  # Compute embedding\n",
    "            \n",
    "            # ✅ Unique identifier (matches whole-case embedding structure)\n",
    "            sentence_id = f\"{case_id}_{idx}\"\n",
    "\n",
    "            all_sentence_embeddings.append(embedding.tolist())\n",
    "            all_sentence_texts.append(decoded_text)\n",
    "            all_sentence_ids.append(sentence_id)\n",
    "\n",
    "    # ✅ Store batch of sentence embeddings in ChromaDB\n",
    "    collection.add(ids=all_sentence_ids, documents=all_sentence_texts, embeddings=all_sentence_embeddings)\n",
    "\n",
    "    print(f\"✅ Sentence embeddings from {batch_file} stored in ChromaDB successfully!\")\n",
    "\n",
    "print(\"\\n🎉 All sentence embeddings processed & stored successfully in ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Retrieved 5 relevant cases!\n",
      "✅ Retrieved 0 sentences within 0 tokens.\n",
      "\n",
      "🔹 Final Retrieved Sentences:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# ✅ Load InLegalBert tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBert\")\n",
    "model = AutoModel.from_pretrained(\"law-ai/InLegalBert\")\n",
    "\n",
    "# ✅ Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Update with actual path\n",
    "case_collection = chroma_client.get_collection(\"legal_docs\")  # Whole-case embeddings\n",
    "sentence_collection = chroma_client.get_collection(\"legal_sentences\")  # Sentence embeddings\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Generate embeddings using InLegalBert\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Extract CLS token\n",
    "\n",
    "def retrieve_dynamic_sentences(query, top_k_cases=5, max_tokens=3000):\n",
    "    \"\"\"\n",
    "    Retrieves top relevant cases and extracts the most relevant sentences while keeping the token count under max_tokens.\n",
    "    \"\"\"\n",
    "    # 🔹 Convert query to embedding using InLegalBert (matches stored format)\n",
    "    query_embedding = embed_text(query)\n",
    "\n",
    "    # 🔹 Retrieve the top K most relevant cases\n",
    "    case_results = case_collection.query(query_embeddings=[query_embedding], n_results=top_k_cases)\n",
    "    \n",
    "    retrieved_cases = case_results[\"documents\"][0]  # Extract top case texts\n",
    "    case_ids = case_results[\"ids\"][0]  # Extract their unique identifiers\n",
    "\n",
    "    print(f\"🔍 Retrieved {len(retrieved_cases)} relevant cases!\")\n",
    "\n",
    "    # 🔹 Retrieve sentences from the selected cases\n",
    "    all_retrieved_sentences = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for case_id in case_ids:\n",
    "        sentence_results = sentence_collection.query(query_embeddings=[query_embedding], n_results=10, where={\"case_id\": case_id})\n",
    "        sentences = sentence_results[\"documents\"][0]  # Extract relevant sentences\n",
    "\n",
    "        for sent in sentences:\n",
    "            sent_tokens = len(tokenizer.tokenize(sent))\n",
    "\n",
    "            if total_tokens + sent_tokens <= max_tokens:\n",
    "                all_retrieved_sentences.append(sent)\n",
    "                total_tokens += sent_tokens\n",
    "            else:\n",
    "                break  # Stop if token limit is reached\n",
    "\n",
    "        if total_tokens >= max_tokens:\n",
    "            break  # Stop if token limit is reached across cases\n",
    "\n",
    "    print(f\"✅ Retrieved {len(all_retrieved_sentences)} sentences within {total_tokens} tokens.\")\n",
    "    return all_retrieved_sentences\n",
    "\n",
    "\n",
    "# ✅ Example Query\n",
    "query = \"What are the legal provisions for forming a new state in India?\"\n",
    "retrieved_sentences = retrieve_dynamic_sentences(query, top_k_cases=5, max_tokens=3000)\n",
    "\n",
    "print(\"\\n🔹 Final Retrieved Sentences:\\n\")\n",
    "for i, sent in enumerate(retrieved_sentences, 1):\n",
    "    print(f\"{i}. {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking stored sentence count in ChromaDB...\n",
      "Total sentences stored: 11970\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Checking stored sentence count in ChromaDB...\")\n",
    "print(f\"Total sentences stored: {sentence_collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Retrieved Sentences (Without Filtering): [[\"petitioner : ramchandra keshav adke & ors vs. respondent : govind joti chavare and ors. date of judgment04 / 03 / 1975 bench : sarkaria, ranjit singh bench : sarkaria, ranjit singh chandrachud, y. v. gupta, a. c. citation : 1975 air 915 1975 scr ( 3 ) 839 1975 scc ( 1 ) 559 act : bombay tenancy act ( 67 of 1948 ) - - s. 5 ( 3 ) - - scope of. headnote : section 5 ( 3 ) ( b ) of the bombay tenancy act enacts that a tenant may terminate the tenancy at any time by surrendering his interest as a tenant in favour of the landlord provided that such surrender shall be in writing and shall be verified before the mamlatdar in the manner prescribed. rule 2 - a of the rules states that the mamlatdar, when verifying a surrender of a tenancy by a tenant, shall satisfy himself after such inquiry as he thinks fit, that the tenant understands the nature and consequences of the surrender and also that it is voluntary, and shall endorse his findings in that behalf upon the document of surrender. the appellants, who were the landlords of certain lands, made an application to the mamlatdar stating that the tenant was willing to surrender his tenancy in the agricultural land and prayed for verification under s. 5 ( 3 ) of the bombay tenancy act ( 67 of 1948 ). the mamlatdar did not verify the surrender. the circle officer recorded the statement of the tenant and the landlords and passed an order on the application. mutation entry was made in the record of rights of the village and the landlords'name was entered in the register as a person in actual possession of the land. a few months later, however, the tenant made an application for a declaration that he was the tenant is possession of the land in dispute. this was dismissed by the tenancy aval karkun. the tenant thereupon preferred an appeal before the special deputy collector, who held that the order passed by the circle officer was not an order passed by the mamlatdar as required by the tenancy law and as such it was without jurisdiction and void and that there was no verification of the surrender application as required by law. the revision application preferred to the maharasthra revenue tribunal by the landlords was dismissed\", 'judgment rajendra nath mittal, j. 1. this reference has been made by the sales tax tribunal under sub - section ( 1 ) of section 22 of the punjab general sales tax act, 1948 ( hereinafter referred to as \" the act \" ). 2. the facts of this case briefly are that m / s. devi dass gopal krishan is a registered dealer under the act and is carrying on the business of ginning of cotton and oil crushing at moga, district ferozepore. in the assessment year 1960 - 61, the assessee filed four returns of purchase tax payable for the quarters ending on 30th june, 1960, 30th september, 1960, 31st december, 1960 and 31st march, 1961, on 28th september, 1961, in which \" nil \" was shown against all the columns, namely, \" a \" to \" h \". five notes were given at the end of the returns which are as follows : ( 1 ) extraction of oil from oil - seeds does not amount to manufacture. ( 2 ) imposition of purchase tax amounts to imposition of excise duty which could be levied by the government of india only. ( 3 ) definition of purchase is defective. ( 4 ) in the matter of purchase tax leave to appeal to the supreme court has been granted by the high court of punjab. ( 5 ) assessee is not liable to pay tax on purchase made through commission agents, as they are not his purchases. 3. mr. k. k. uppal, who was the assessing authority of punjab ( enforcement officer, punjab ), created an additional demand of rs. 35, 041. 23 for the assessment year 1960 - 61 by his order dated 21st march, 1963. the assessee having felt aggrieved against the said order filed an appeal to the deputy excise and taxation commissioner, jullundur division, jullundur, which was decided by him on 9th november, 1964, by which he remanded the case to the assessing authority to decide it afresh according to the observations made in the order. the case came up for decision before shri sarbjit singh, assessing authority, ferozepore, who issued fresh notice to the assessee on 29th july, 1965, for appearance before him on 16th august, 1965. he made the assessment, afresh and created an additional demand of rs. 38, 493. 80 by', 'judgment rajendra nath mittal, j. 1. this reference has been made by the sales tax tribunal under sub - section ( 1 ) of section 22 of the punjab general sales tax act, 1948 ( hereinafter referred to as \" the act \" ). 2. the facts of this case briefly are that m / s. devi dass gopal krishan is a registered dealer under the act and is carrying on the business of ginning of cotton and oil crushing at moga, district ferozepore. in the assessment year 1960 - 61, the assessee filed four returns of purchase tax payable for the quarters ending on 30th june, 1960, 30th september, 1960, 31st december, 1960 and 31st march, 1961, on 28th september, 1961, in which \" nil \" was shown against all the columns, namely, \" a \" to \" h \". five notes were given at the end of the returns which are as follows : ( 1 ) extraction of oil from oil - seeds does not amount to manufacture. ( 2 ) imposition of purchase tax amounts to imposition of excise duty which could be levied by the government of india only. ( 3 ) definition of purchase is defective. ( 4 ) in the matter of purchase tax leave to appeal to the supreme court has been granted by the high court of punjab. ( 5 ) assessee is not liable to pay tax on purchase made through commission agents, as they are not his purchases. 3. mr. k. k. uppal, who was the assessing authority of punjab ( enforcement officer, punjab ), created an additional demand of rs. 35, 041. 23 for the assessment year 1960 - 61 by his order dated 21st march, 1963. the assessee having felt aggrieved against the said order filed an appeal to the deputy excise and taxation commissioner, jullundur division, jullundur, which was decided by him on 9th november, 1964, by which he remanded the case to the assessing authority to decide it afresh according to the observations made in the order. the case came up for decision before shri sarbjit singh, assessing authority, ferozepore, who issued fresh notice to the assessee on 29th july, 1965, for appearance before him on 16th august, 1965. he made the assessment, afresh and created an additional demand of rs. 38, 493. 80 by', \"judgment monjula bose, j. 1. the suit instituted by rose simpson the petitioner under the indian divorce act is primarily for a declaration that her marriage with the respondent is a nullity. the parties profess the christian religion. in the petition it is, inter alia, alleged that at the time of the impugned marriage on may 7th, 1955, the respondent represented to the petitioner that his prior marriage with the petitioner's eldest sister agnes simpson had been lawfully dissolved. subsequently, in a suit for divorce on charges of adultery, initiated by the petitioner against her husband being suit no. 9, 1973 ( mrs. rose biswas v. binimoy biswas ) she came to know that the prior marriage of her eldest sister with her husband, the present respondent had not been dissolved. on advice the petitioner obtained leave to withdraw the said suit with liberty to institute appropriate proceedings. the present suit was caused to be filed thereafter. it is contended that inasmuch as the said prior marriage of the respondent was in force at the material time, the subsequent marriage between the petitioner and the respondent is a nullity. the parties have four boys and three girls some of whom were born before the ceremony of marriage. the eldest two daughters aged 26 years and 24 years are married and the youngest daughter is a minor aged 13 years. 2. in the written statement filed by the respondent it is denied that the respondent's earlier marriage was in force at the time of his marriage with the petitioner. it is contended that the present suit is false and frivolous and instituted in collusion and connivance with persons with whom, the petitioner is living in adultery. 3. the issues settled for trial were : - - ( 1 ) is the marriage of the petitioner and the respondent a nullity as alleged in para 11 of the petition? ( 2 ) to what relief, if any, is the petitioner entitled? 4. the petitioner offered herself for examination. briefly stated, her evidence is that she was married to the respondent on the 7th may, 1955. the certificate of marriage was tendered as exbt. a / 1. the respondent was previously married first to one tatini biswas, and thereafter on june 4, 1947 to her eldest sister agnes simpson. at the time of marriage with her, the respondent had represented that he was divorced from agnes. the certificate of marriage between the\", 'judgment v. s. deshpande, j. ( 1 ) under clause ( e ) of the proviso to sub - section ( 1 ) of section 14 of the delhi rent control act, 1958 ( hereinafter called the act ) the controller may, on an application by the landlord, order the eviction of the tenant on the ground that : - ( 1 ) the premises are required bona fide by the landlord for the occupation of himself and / or his family, and ( 2 ) the landlord has no other reasonably suitable residential accommodation. ( 2 ) the decision of this appeal by the tenant under section 39 of the act depends upon the correct construction of these two provisions and their inter - connection, on the following facts : \" thelandlord respondent is a grade iii central government servant, his pay being rs. 600. 00 per month. he owns two houses, one in nizamuddin and the other in jangpura. he occupies the ground floor of the nizamuddin house. in column 14 of the petition for eviction he stated that the appellant was his tenant occupying the first floor of the nizamuddin house from 1 - 3 - 1966 as per court decision. in para 6 of the written statement, the tenant stated that he was a tenant of the premises from 1 - 8 - 1961 but the landlord tried to evict him in case no. 961 of 1964 decided on 19 - 3 - 1966 by the controller. but the tenant got the standard rent of the house fixed in that case. no rejoinder was filed by the landlord and the argument proceeded on the basis that the tenant was in occupation of the premises from 1961 onwards. the landlord applied for the eviction of the tenaat in june 1969 on the ground that he required the premises bona fide for the occupation of himself and his family which consisted of his wife and five children. the tenant resisted the petition on the ground that the accommodation with the landlord on the ground floor of the house was sufficient as the daughter of the landlord was expected to be married soon while one son of the landlord was living in a hostel. the tenant also pointed out that the landlord had purchased another house at jangpura which he let out. by subsequent affidavits of the parties it was stated that while the daughter of the landlord has since then been married the son of the landlord had also been married. the income of the landlord from rent of the houses is said to be rs. 1000. 00 per month. as the', 'judgment v. s. deshpande, j. ( 1 ) under clause ( e ) of the proviso to sub - section ( 1 ) of section 14 of the delhi rent control act, 1958 ( hereinafter called the act ) the controller may, on an application by the landlord, order the eviction of the tenant on the ground that : - ( 1 ) the premises are required bona fide by the landlord for the occupation of himself and / or his family, and ( 2 ) the landlord has no other reasonably suitable residential accommodation. ( 2 ) the decision of this appeal by the tenant under section 39 of the act depends upon the correct construction of these two provisions and their inter - connection, on the following facts : \" thelandlord respondent is a grade iii central government servant, his pay being rs. 600. 00 per month. he owns two houses, one in nizamuddin and the other in jangpura. he occupies the ground floor of the nizamuddin house. in column 14 of the petition for eviction he stated that the appellant was his tenant occupying the first floor of the nizamuddin house from 1 - 3 - 1966 as per court decision. in para 6 of the written statement, the tenant stated that he was a tenant of the premises from 1 - 8 - 1961 but the landlord tried to evict him in case no. 961 of 1964 decided on 19 - 3 - 1966 by the controller. but the tenant got the standard rent of the house fixed in that case. no rejoinder was filed by the landlord and the argument proceeded on the basis that the tenant was in occupation of the premises from 1961 onwards. the landlord applied for the eviction of the tenaat in june 1969 on the ground that he required the premises bona fide for the occupation of himself and his family which consisted of his wife and five children. the tenant resisted the petition on the ground that the accommodation with the landlord on the ground floor of the house was sufficient as the daughter of the landlord was expected to be married soon while one son of the landlord was living in a hostel. the tenant also pointed out that the landlord had purchased another house at jangpura which he let out. by subsequent affidavits of the parties it was stated that while the daughter of the landlord has since then been married the son of the landlord had also been married. the income of the landlord from rent of the houses is said to be rs. 1000. 00 per month. as the', 'judgment seetharama reddy, j. 1. first defendant is the appellant. union of india filed the suit o. s. no. 256 of 1971 on the file of city civil court, hyderabad for recovery of rs. 46, 644 - 75 being the balance of bank guarantee payable by the first defendant to the plaintiff, against the first defendant ; and in the alternative, against defendants 1 and 2 and for subsequent interest from the date of the suit at 6 % per annum till the date of recovery and for costs. 2. the plaint averments are : - - the second defendant was appointed by the plaintiff as the sole advertising agent to procure advertisements in the telephone directories for a total of five issues commencing from july, 1965. an agreement was entered into between the plaintiff and the second defendant on 6 - 4 - 1965. as per clause 4 of the said agreement the second defendant gave bank guarantee by the first defendant bank on 30 - 4 - 1965 for the due performance by him of the terms of the agreement. as per the bank guarantee, the bank is liable for a sum not exceeding rs. 47, 325 - 75 if the second defendant failed to perform his part of the contract. the issues of the directory were published during july, 1965, january, 1966, aug., 1966, feb., 1967 and sept., 1967. as per clause 5 of the agreement the plaintiff will pay to the sole advertising agent a commission of twenty - five per cent on rs. 63, 101 / - of the minimum gross revenue guaranteed by the sole advertising agent and in no case the minimum net share of the government revenue should be less than rs. 47, 325 - 75. the plaintiff although has fixed the minimum rates chargeable by the second defendant, has not fixed the maximum rates that could be charged from the advertisers by him and so the second defendant is empowered to collect any higher rate than the minimum rates prescribed. in respect of collections over and above the gross minimum revenue of rs. 63, 101 / - the second defendant is allowed an additional commission of rs. 15 / - per cent on such additional collections. the second defendant is also under an obligation to maintain proper accounts for these sums along with the concerned vouchers and to allow the posts and telegraphs department to audit such accounts as per clause 12 of the agreement. the second defendant submitted his revenue statements for all the issues as required in clause 7 of the agreement. the arithmetical discrepancies in', 'petitioner : nirmaljit singh hoon vs. respondent : the state of west bengal and anr. date of judgment06 / 09 / 1972 bench : shelat, j. m. bench : shelat, j. m. dua, i. d. khanna, hans raj citation : 1972 air 2639 1973 scr ( 2 ) 66 1973 scc ( 3 ) 753 citator info : rf 1976 sc1672 ( 15 ) r 1977 sc2018 ( 5 ) rf 1979 sc 437 ( 8 ) d 1981 sc 22 ( 20, 21 ) rf 1986 sc2045 ( 45 ) act : code of criminal procedure ( act 5 of 1898 ), ss. 156 ( 3 ), 195 ( 1 ) ( c ), 202 and 204 - scope of. headnote : h - company ( in voluntary liquidation ) was the owner of 51 % of the, shares in t - company and 707 shares out of them were in the possession, of t - company. the 5th respondent owned the balance of 49 % shares. in a suit filed by him against the h - company the high court passed a decree directing h - company to deliver the 51 % shares to him on payment of a certain sum and issued an injunction restraining h - company, until delivery of the shares, from exercising its rights as holder of those : shares. some time later one of the liquidators, v, of h - company, and m went to the office of t - company where v executed a receipt and an indemnity bond. the receipt recorded the fact that the 707 share certificates were received from the 2nd respondent one of the directors of the t - company. it also contained two endorsements ; one in the handwriting of the 2nd respondent stating \" shares with me \" and another, addressed to the 2nd respondent alleged to. have been written by v, stating, \" i do not want to carry these with me, hence leaving meantime with personally for delivery to me later \". the indemnity bond purported to indemnify t - company against any claims by the 5th respondent in respect of the 707 shares and contained also certain undertakings. h - company took out execution against t - company for the delivery of the 707 shares claiming entrustment of the shares to the second respondent by v. copies of the receipt and the indemnity bond were filed, and the originals were shown to the counsel for t - company, during the proceedings for satisfying them that the copies were', \"order 1. this is a petition under article 226 of the constitution praying for the quashing of an order dated 16 - 8 - 1982 passed by the commissioner, agra division, agra ( respondent no. 1 ). by that order, the respondent no. 1 has set aside an auction sale held in favour of the petitioner in respect of a factory owned by m / s. indo - swedish pipe manufacturers limited, nawalganj ( agra ). the sale took place under the provisions of the u. p. zamindari abolition and land reforms rules in pursuance of a recovery certificate dated 31 - 3 - 1979 issued by the managing director of the u. p. financial corporation ( respondent no. 3 ) to the collector. agra ( respondent no. 2 ) for the recovery of a sum of rs. 19, 96, 652. 79 from m / s. indo - swedish pipe manufacturers limited. nawalganj ( agra ) under the provisions of the uttar pradesh public moneys ( recovery of dues ) act, 1972, the managing director of the u. p. financial corporation in his letter dated 31 - 3 - 1979 mentioned details of the landed property, machinery of the factory sought to be auctioned and details of other properties of the defaulter company. they were marked as schedules a, b and c to the said letter. the collector, agra, in pursuance of the recovery certificate, attached the property of the defaulter firm, issued a sale proclamation dated 26 - 1 - 1979, which was served on the manager of the indo - swedish pipe manufacturers on 6 - 8 - 1979. according to the commissioner, agra division, agra, this proclamation included only the landed property, and not the plant and machinery standing on the land. it appears that another proclamation was also issued which included the entire factory including the plant and machinery standing'thereon. the proclamation of sale was affixed on the gate of the factory on 13 - 11 - 1979, and publicity thereof was also made by pasting a copy of the same on the notice boards of the nagar mahapalika, agra and the collectorate, agra. publicity was also made by beat of drum in the industrial area of nunihaee and hathras road, agra on 22 - 12 - 1979. the sale took place on 28 - 12 - 1979 on the premises of the\", \"order 1. this is a petition under article 226 of the constitution praying for the quashing of an order dated 16 - 8 - 1982 passed by the commissioner, agra division, agra ( respondent no. 1 ). by that order, the respondent no. 1 has set aside an auction sale held in favour of the petitioner in respect of a factory owned by m / s. indo - swedish pipe manufacturers limited, nawalganj ( agra ). the sale took place under the provisions of the u. p. zamindari abolition and land reforms rules in pursuance of a recovery certificate dated 31 - 3 - 1979 issued by the managing director of the u. p. financial corporation ( respondent no. 3 ) to the collector. agra ( respondent no. 2 ) for the recovery of a sum of rs. 19, 96, 652. 79 from m / s. indo - swedish pipe manufacturers limited. nawalganj ( agra ) under the provisions of the uttar pradesh public moneys ( recovery of dues ) act, 1972, the managing director of the u. p. financial corporation in his letter dated 31 - 3 - 1979 mentioned details of the landed property, machinery of the factory sought to be auctioned and details of other properties of the defaulter company. they were marked as schedules a, b and c to the said letter. the collector, agra, in pursuance of the recovery certificate, attached the property of the defaulter firm, issued a sale proclamation dated 26 - 1 - 1979, which was served on the manager of the indo - swedish pipe manufacturers on 6 - 8 - 1979. according to the commissioner, agra division, agra, this proclamation included only the landed property, and not the plant and machinery standing on the land. it appears that another proclamation was also issued which included the entire factory including the plant and machinery standing'thereon. the proclamation of sale was affixed on the gate of the factory on 13 - 11 - 1979, and publicity thereof was also made by pasting a copy of the same on the notice boards of the nagar mahapalika, agra and the collectorate, agra. publicity was also made by beat of drum in the industrial area of nunihaee and hathras road, agra on 22 - 12 - 1979. the sale took place on 28 - 12 - 1979 on the premises of the\"]]\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the legal provisions for forming a new state in India?\"\n",
    "query_embedding = embed_text(query)\n",
    "sentence_results = sentence_collection.query(query_embeddings=[query_embedding], n_results=10)\n",
    "print(\"\\n📌 Retrieved Sentences (Without Filtering):\", sentence_results[\"documents\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
